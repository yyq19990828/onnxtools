# Quick Start: OCR Metrics Evaluation

**Feature**: 006-make-ocr-metrics | **Date**: 2025-10-10

## 5åˆ†é’Ÿå¿«é€Ÿä¸Šæ‰‹

### 1. å®‰è£…ä¾èµ–

```bash
# ä½¿ç”¨uvï¼ˆæ¨èï¼‰
uv add python-Levenshtein

# æˆ–ä½¿ç”¨pip
pip install python-Levenshtein
```

### 2. å‡†å¤‡æ•°æ®é›†

ç¡®ä¿æ‚¨çš„æ•°æ®é›†éµå¾ªä»¥ä¸‹ç»“æ„ï¼š

```
dataset/
â”œâ”€â”€ images/
â”‚   â”œâ”€â”€ train_word_1.png
â”‚   â”œâ”€â”€ train_word_2.png
â”‚   â””â”€â”€ ...
â”œâ”€â”€ train.txt          # æ ‡ç­¾æ–‡ä»¶
â””â”€â”€ val.txt            # å¯é€‰ï¼šéªŒè¯é›†æ ‡ç­¾
```

**æ ‡ç­¾æ–‡ä»¶æ ¼å¼** (train.txt):
```
images/train_word_1.png	äº¬A12345
images/train_word_2.png	æ²ªB67890
images/train_word_3.png	ç²¤C11111
```

> **æ³¨æ„**: æ¯è¡Œç”¨Tabï¼ˆ`\t`ï¼‰åˆ†éš”å›¾åƒè·¯å¾„å’Œæ ‡ç­¾æ–‡æœ¬

### 3. åŸºç¡€ä½¿ç”¨

```python
from infer_onnx import OCRONNX, OCRDatasetEvaluator

# åŠ è½½OCRæ¨¡å‹
ocr_model = OCRONNX(
    onnx_path='models/ocr.onnx',
    character=character_dict,  # å­—ç¬¦å­—å…¸
    conf_thres=0.5
)

# åˆ›å»ºè¯„ä¼°å™¨
evaluator = OCRDatasetEvaluator(ocr_model)

# è¯„ä¼°æ•°æ®é›†ï¼ˆé»˜è®¤è¡¨æ ¼è¾“å‡ºï¼‰
results = evaluator.evaluate_dataset(
    label_file='/path/to/dataset/train.txt',
    dataset_base_path='/path/to/dataset',
    conf_threshold=0.5
)
```

**è¾“å‡ºç¤ºä¾‹**:
```
å¼€å§‹è¯„ä¼°OCRæ•°æ®é›†ï¼Œå…± 1000 å¼ å›¾åƒ
å¤„ç†è¿›åº¦: 50/1000 (5.0%)
å¤„ç†è¿›åº¦: 100/1000 (10.0%)
...
å¤„ç†è¿›åº¦: 1000/1000 (100.0%)

æŒ‡æ ‡                  å®Œå…¨å‡†ç¡®ç‡        å½’ä¸€åŒ–ç¼–è¾‘è·ç¦»      ç¼–è¾‘è·ç¦»ç›¸ä¼¼åº¦
OCRè¯„ä¼°              0.925           0.045           0.955

ç»Ÿè®¡ä¿¡æ¯              æ€»æ ·æœ¬æ•°          è¯„ä¼°æ•°            è¿‡æ»¤æ•°            è·³è¿‡æ•°
æ ·æœ¬ç»Ÿè®¡              1000            980             15              5
```

---

## é«˜çº§ç”¨æ³•

### 1. JSONå¯¼å‡ºæ¨¡å¼

```python
# å¯¼å‡ºä¸ºJSONæ ¼å¼
results = evaluator.evaluate_dataset(
    label_file='/path/to/dataset/val.txt',
    dataset_base_path='/path/to/dataset',
    output_format='json'  # 'table' æˆ– 'json'
)

# ä¿å­˜åˆ°æ–‡ä»¶
import json
with open('evaluation_results.json', 'w', encoding='utf-8') as f:
    json.dump(results, f, indent=2, ensure_ascii=False)
```

**JSONè¾“å‡ºç¤ºä¾‹**:
```json
{
  "accuracy": 0.925,
  "normalized_edit_distance": 0.045,
  "edit_distance_similarity": 0.955,
  "total_samples": 1000,
  "evaluated_samples": 980,
  "filtered_samples": 15,
  "skipped_samples": 5,
  "evaluation_time": 245.3,
  "avg_inference_time_ms": 12.5
}
```

### 2. ç½®ä¿¡åº¦é˜ˆå€¼ä¼˜åŒ–

```python
# æµ‹è¯•ä¸åŒç½®ä¿¡åº¦é˜ˆå€¼
thresholds = [0.3, 0.5, 0.7, 0.9]
for threshold in thresholds:
    results = evaluator.evaluate_dataset(
        label_file='/path/to/dataset/val.txt',
        dataset_base_path='/path/to/dataset',
        conf_threshold=threshold
    )
    print(f"Threshold {threshold}: Accuracy={results['accuracy']:.3f}")
```

### 3. å¿«é€Ÿæµ‹è¯•ï¼ˆé™åˆ¶å›¾åƒæ•°é‡ï¼‰

```python
# ä»…è¯„ä¼°å‰100å¼ å›¾åƒ
results = evaluator.evaluate_dataset(
    label_file='/path/to/dataset/train.txt',
    dataset_base_path='/path/to/dataset',
    max_images=100  # å¿«é€Ÿæµ‹è¯•
)
```

### 4. å‘½ä»¤è¡Œå·¥å…·ï¼ˆæ¨èï¼‰

```bash
# ä½¿ç”¨eval_ocr.pyè„šæœ¬
python -m infer_onnx.eval_ocr \
    --label-file /path/to/dataset/train.txt \
    --dataset-base /path/to/dataset \
    --ocr-model models/ocr.onnx \
    --conf-threshold 0.5 \
    --output-format table
```

---

## å¸¸è§åœºæ™¯

### åœºæ™¯1: æ¨¡å‹A/Bæµ‹è¯•

```python
from infer_onnx import OCRONNX, OCRDatasetEvaluator

# è¯„ä¼°æ¨¡å‹A
model_a = OCRONNX('models/ocr_v1.onnx', character=char_dict)
evaluator_a = OCRDatasetEvaluator(model_a)
results_a = evaluator_a.evaluate_dataset(
    label_file='dataset/val.txt',
    dataset_base_path='dataset'
)

# è¯„ä¼°æ¨¡å‹B
model_b = OCRONNX('models/ocr_v2.onnx', character=char_dict)
evaluator_b = OCRDatasetEvaluator(model_b)
results_b = evaluator_b.evaluate_dataset(
    label_file='dataset/val.txt',
    dataset_base_path='dataset'
)

# æ¯”è¾ƒç»“æœ
print(f"Model A accuracy: {results_a['accuracy']:.3f}")
print(f"Model B accuracy: {results_b['accuracy']:.3f}")
improvement = (results_b['accuracy'] - results_a['accuracy']) * 100
print(f"Improvement: {improvement:+.2f}%")
```

### åœºæ™¯2: è·¨æ•°æ®é›†è¯„ä¼°

```python
datasets = {
    'train': 'dataset/train.txt',
    'val': 'dataset/val.txt',
    'test': 'dataset/test.txt'
}

for split_name, label_file in datasets.items():
    results = evaluator.evaluate_dataset(
        label_file=label_file,
        dataset_base_path='dataset'
    )
    print(f"{split_name}: Accuracy={results['accuracy']:.3f}, "
          f"Edit Distance Similarity={results['edit_distance_similarity']:.3f}")
```

### åœºæ™¯3: é”™è¯¯åˆ†æï¼ˆä¿å­˜è¯¦ç»†ç»“æœï¼‰

```python
# å¯¼å‡ºæ¯ä¸ªæ ·æœ¬çš„è¯¦ç»†ç»“æœ
results = evaluator.evaluate_dataset(
    label_file='dataset/val.txt',
    dataset_base_path='dataset',
    output_format='json'
)

# åˆ†æé”™è¯¯æ ·æœ¬
import json
with open('detailed_results.json', 'w', encoding='utf-8') as f:
    json.dump(results, f, indent=2, ensure_ascii=False)

# æ‰¾å‡ºè¯†åˆ«é”™è¯¯çš„æ ·æœ¬
if 'per_sample_results' in results:
    errors = [s for s in results['per_sample_results'] if not s['is_correct']]
    print(f"Found {len(errors)} errors")
    for e in errors[:10]:  # æ˜¾ç¤ºå‰10ä¸ªé”™è¯¯
        print(f"GT: {e['ground_truth']} -> Pred: {e['predicted_text']} "
              f"(ED: {e['edit_distance']})")
```

---

## æ€§èƒ½ä¼˜åŒ–å»ºè®®

### 1. GPUåŠ é€Ÿ

ç¡®ä¿OCRæ¨¡å‹ä½¿ç”¨GPUæ¨ç†ï¼š

```python
import onnxruntime as ort

# æ£€æŸ¥å¯ç”¨çš„execution providers
print(ort.get_available_providers())  # åº”åŒ…å« 'CUDAExecutionProvider'

# OCRONNXä¼šè‡ªåŠ¨ä½¿ç”¨GPUï¼ˆå¦‚æœå¯ç”¨ï¼‰
ocr_model = OCRONNX('models/ocr.onnx', character=char_dict)
```

### 2. æ‰¹é‡è¯„ä¼°

å¯¹äºå¤§æ•°æ®é›†ï¼Œå»ºè®®åˆ†æ‰¹è¯„ä¼°ï¼š

```python
# åˆ†æ‰¹è¯„ä¼°ï¼ˆé¿å…å†…å­˜æº¢å‡ºï¼‰
batch_size = 500
total_samples = 5000

for i in range(0, total_samples, batch_size):
    results = evaluator.evaluate_dataset(
        label_file='dataset/train.txt',
        dataset_base_path='dataset',
        max_images=batch_size,
        skip_samples=i  # è·³è¿‡å‰iä¸ªæ ·æœ¬
    )
    # ç´¯ç§¯ç»“æœ...
```

### 3. è¿›åº¦ç›‘æ§

```python
import logging

# å¯ç”¨è¯¦ç»†æ—¥å¿—
logging.basicConfig(level=logging.INFO)

# è¯„ä¼°æ—¶ä¼šè‡ªåŠ¨æ˜¾ç¤ºè¿›åº¦
results = evaluator.evaluate_dataset(
    label_file='dataset/train.txt',
    dataset_base_path='dataset'
)
# è¾“å‡º: å¤„ç†è¿›åº¦: 50/1000 (5.0%)
#       å¤„ç†è¿›åº¦: 100/1000 (10.0%)
#       ...
```

---

## æ•…éšœæ’é™¤

### é—®é¢˜1: "Label file not found"

**åŸå› **: æ ‡ç­¾æ–‡ä»¶è·¯å¾„ä¸æ­£ç¡®

**è§£å†³**:
```python
from pathlib import Path

label_file = Path('/path/to/dataset/train.txt')
assert label_file.exists(), f"Label file not found: {label_file}"
```

### é—®é¢˜2: ä¸­æ–‡æ˜¾ç¤ºä¹±ç 

**åŸå› **: ç»ˆç«¯ä¸æ”¯æŒUTF-8ç¼–ç 

**è§£å†³**:
```bash
# Linux/Mac
export LANG=en_US.UTF-8

# Windows (PowerShell)
chcp 65001
```

### é—®é¢˜3: è¯„ä¼°é€Ÿåº¦æ…¢

**å¯èƒ½åŸå› **:
1. CPUæ¨ç†ï¼ˆåº”ä½¿ç”¨GPUï¼‰
2. å›¾åƒåŠ è½½IOç“¶é¢ˆ
3. æ•°æ®é›†è¿‡å¤§

**è§£å†³**:
```python
# 1. ç¡®è®¤GPUå¯ç”¨
import onnxruntime as ort
assert 'CUDAExecutionProvider' in ort.get_available_providers()

# 2. ä½¿ç”¨max_imagesé™åˆ¶æµ‹è¯•è§„æ¨¡
results = evaluator.evaluate_dataset(..., max_images=100)

# 3. æ£€æŸ¥å›¾åƒåˆ†è¾¨ç‡ï¼ˆè¿‡å¤§çš„å›¾åƒä¼šæ…¢ï¼‰
```

### é—®é¢˜4: "KeyError: 'accuracy'"

**åŸå› **: è¯„ä¼°å¤±è´¥æˆ–è¿”å›ç©ºç»“æœ

**è§£å†³**:
```python
results = evaluator.evaluate_dataset(...)

# å®‰å…¨è®¿é—®ç»“æœ
accuracy = results.get('accuracy', 0.0)
if accuracy == 0.0:
    print("Warning: No valid evaluations performed")
```

---

## ä¸ç›®æ ‡æ£€æµ‹è¯„ä¼°å¯¹æ¯”

| ç‰¹æ€§ | ç›®æ ‡æ£€æµ‹ (eval_coco.py) | OCRè¯„ä¼° (eval_ocr.py) |
|-----|------------------------|---------------------|
| **è¾“å…¥æ ¼å¼** | YOLOæ ¼å¼ (images/, labels/) | Tabåˆ†éš”çš„label list (train.txt) |
| **æ ¸å¿ƒæŒ‡æ ‡** | mAP, Precision, Recall | å®Œå…¨å‡†ç¡®ç‡, ç¼–è¾‘è·ç¦» |
| **è¾“å‡ºæ ¼å¼** | è¡¨æ ¼å¯¹é½ï¼ˆä¸­æ–‡æ”¯æŒï¼‰ | è¡¨æ ¼å¯¹é½ + JSONå¯¼å‡º |
| **æ¶æ„æ¨¡å¼** | DatasetEvaluatorç±» | OCRDatasetEvaluatorç±» |
| **æ€§èƒ½ç›®æ ‡** | <5åˆ†é’Ÿ/1000å›¾ï¼ˆGPUï¼‰ | <5åˆ†é’Ÿ/1000å›¾ï¼ˆGPUï¼‰ |
| **æ—¥å¿—è¿›åº¦** | æ¯100å¼  | æ¯50å¼  |

---

## APIå‚è€ƒ

### OCRDatasetEvaluator

```python
class OCRDatasetEvaluator:
    def __init__(self, ocr_model: OCRONNX):
        """åˆå§‹åŒ–è¯„ä¼°å™¨"""

    def evaluate_dataset(
        self,
        label_file: str,
        dataset_base_path: str,
        conf_threshold: float = 0.5,
        max_images: Optional[int] = None,
        output_format: str = 'table'
    ) -> Dict[str, Any]:
        """è¯„ä¼°OCRæ•°æ®é›†

        Args:
            label_file: æ ‡ç­¾æ–‡ä»¶è·¯å¾„
            dataset_base_path: æ•°æ®é›†æ ¹ç›®å½•
            conf_threshold: ç½®ä¿¡åº¦é˜ˆå€¼ [0, 1]
            max_images: æœ€å¤§è¯„ä¼°å›¾åƒæ•°
            output_format: 'table' æˆ– 'json'

        Returns:
            è¯„ä¼°ç»“æœå­—å…¸
        """
```

### è¿”å›å€¼ç»“æ„

```python
{
    'accuracy': float,                    # å®Œå…¨å‡†ç¡®ç‡ [0, 1]
    'normalized_edit_distance': float,    # å½’ä¸€åŒ–ç¼–è¾‘è·ç¦» [0, 1]
    'edit_distance_similarity': float,    # ç¼–è¾‘è·ç¦»ç›¸ä¼¼åº¦ [0, 1]
    'total_samples': int,                 # æ€»æ ·æœ¬æ•°
    'evaluated_samples': int,             # è¯„ä¼°æ ·æœ¬æ•°
    'filtered_samples': int,              # è¿‡æ»¤æ ·æœ¬æ•°
    'skipped_samples': int,               # è·³è¿‡æ ·æœ¬æ•°
    'evaluation_time': float,             # è¯„ä¼°æ—¶é—´ï¼ˆç§’ï¼‰
    'avg_inference_time_ms': float        # å¹³å‡æ¨ç†æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰
}
```

---

## ä¸‹ä¸€æ­¥

- ğŸ“– é˜…è¯» [data-model.md](./data-model.md) äº†è§£æ•°æ®æ¨¡å‹
- ğŸ“‹ æŸ¥çœ‹ [contracts/ocr_evaluator_api.yaml](./contracts/ocr_evaluator_api.yaml) APIåˆçº¦
- ğŸ§ª è¿è¡Œåˆçº¦æµ‹è¯•: `pytest tests/contract/test_ocr_evaluator_contract.py`
- ğŸ“Š æŸ¥çœ‹ [research.md](./research.md) æŠ€æœ¯å†³ç­–

---

**æœ€åæ›´æ–°**: 2025-10-10 | **ç»´æŠ¤è€…**: ONNX Vehicle Plate Recognition Team
