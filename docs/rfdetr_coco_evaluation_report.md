# RF-DETR COCOè¯„ä¼°å®ç°è¯¦ç»†åˆ†ææŠ¥å‘Š

> **åˆ†ææ—¶é—´**: 2025-10-11
> **é¡¹ç›®**: ONNXè½¦è¾†ç‰Œç…§è¯†åˆ«ç³»ç»Ÿ
> **ç›®æ ‡ä»“åº“**: `third_party/rfdetr`
> **åˆ†æèŒƒå›´**: COCOæŒ‡æ ‡è®¡ç®—æµç¨‹ã€æ¨¡å‹è¾“å‡ºæ ¼å¼ã€æ•°æ®æ¡¥æ¥æœºåˆ¶

---

## 1. æ‰§è¡Œæ‘˜è¦ (Executive Summary)

æœ¬æŠ¥å‘Šæ·±å…¥åˆ†æäº†RF-DETRä»“åº“ä¸­COCOè¯„ä¼°æŒ‡æ ‡çš„å®Œæ•´å®ç°æµç¨‹ï¼Œæ¶µç›–ä»¥ä¸‹æ ¸å¿ƒå†…å®¹ï¼š

- âœ… **æ¨¡å‹è¾“å‡ºæ ¼å¼**: `pred_logits` (åˆ†ç±») + `pred_boxes` (è¾¹æ¡†) + `pred_masks` (å¯é€‰åˆ†å‰²)
- âœ… **åå¤„ç†æµç¨‹**: Sigmoidæ¿€æ´» â†’ Top-Ké€‰æ‹© â†’ è¾¹æ¡†æ ¼å¼è½¬æ¢ (cxcywhâ†’xyxy) â†’ ç»å¯¹åæ ‡ç¼©æ”¾
- âœ… **COCOæ¡¥æ¥**: è‡ªå®šä¹‰æ ¼å¼ â†’ COCOæ ‡å‡†JSON â†’ pycocotoolsåŠ è½½ â†’ è¯„ä¼°æŒ‡æ ‡è®¡ç®—
- âœ… **è¯„ä¼°æŒ‡æ ‡**: 12ä¸ªæ ‡å‡†COCOæŒ‡æ ‡ (AP/AR) + 4ä¸ªæ‰©å±•æŒ‡æ ‡ (Precision/Recall/F1/Per-Class)

**å…³é”®å‘ç°**:
1. RF-DETRé‡‡ç”¨**ä¸‰é˜¶æ®µæ•°æ®è½¬æ¢**ç¡®ä¿ä¸COCO APIå…¼å®¹
2. ä½¿ç”¨**CocoEvaluator**ç±»å°è£…å®Œæ•´è¯„ä¼°æµç¨‹ï¼Œæ”¯æŒåˆ†å¸ƒå¼è®­ç»ƒ
3. æä¾›**æ‰©å±•æŒ‡æ ‡è®¡ç®—**å‡½æ•°ï¼Œè‡ªåŠ¨æ‰¾åˆ°æœ€ä½³F1é˜ˆå€¼å¹¶åˆ†è§£åˆ°æ¯ä¸ªç±»åˆ«

---

## 2. æ ¸å¿ƒè¯„ä¼°æ–‡ä»¶æ¦‚è§ˆ

### 2.1 ä¸»è¦è¯„ä¼°ç›¸å…³æ–‡ä»¶

| æ–‡ä»¶è·¯å¾„ | èŒè´£ | å…³é”®ç±»/å‡½æ•° | ä»£ç è¡Œæ•° |
|---------|------|------------|---------|
| `datasets/coco_eval.py` | COCOè¯„ä¼°å™¨æ ¸å¿ƒå®ç° | `CocoEvaluator` | 272è¡Œ |
| `engine.py` | è®­ç»ƒå’Œè¯„ä¼°å¼•æ“ | `evaluate()`, `coco_extended_metrics()` | 341è¡Œ |
| `models/lwdetr.py` | PostProcessåå¤„ç†ç±» | `PostProcess.forward()` | ~1000+è¡Œ |
| `datasets/coco.py` | COCOæ•°æ®é›†åŠ è½½ | `CocoDetection` | 330è¡Œ |
| `util/metrics.py` | æŒ‡æ ‡è®¡ç®—å’Œå¯è§†åŒ– | `log_coco_metrics()` | 243è¡Œ |
| `detr.py` | RF-DETRé«˜çº§API | `RFDetr`, `train()`, `val()` | 477è¡Œ |

### 2.2 ä¾èµ–å…³ç³»å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   pycocotools (å¤–éƒ¨ä¾èµ–)                 â”‚
â”‚   - COCO (GTæ•°æ®åŠ è½½)                    â”‚
â”‚   - COCOeval (è¯„ä¼°å¼•æ“)                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚ ä¾èµ–
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   datasets/coco_eval.py                 â”‚
â”‚   - CocoEvaluator (å°è£…è¯„ä¼°æµç¨‹)         â”‚
â”‚   - prepare_for_coco_detection()        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚ è°ƒç”¨
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   engine.py                             â”‚
â”‚   - evaluate() (ä¸»è¯„ä¼°å¾ªç¯)              â”‚
â”‚   - coco_extended_metrics() (æ‰©å±•æŒ‡æ ‡)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚ ä½¿ç”¨
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   models/lwdetr.py                      â”‚
â”‚   - PostProcess (åå¤„ç†æ¨¡å—)             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 3. æ¨¡å‹è¾“å‡ºæ•°æ®æ ¼å¼

### 3.1 åŸå§‹æ¨¡å‹è¾“å‡º (Raw Model Output)

RF-DETRæ¨¡å‹çš„ç›´æ¥è¾“å‡ºæ˜¯ä¸€ä¸ªå­—å…¸ï¼ŒåŒ…å«ä»¥ä¸‹å¼ é‡ï¼š

```python
# æ–‡ä»¶: rfdetr/models/lwdetr.py - LWDetr.forward()
model_outputs = {
    # åˆ†ç±»é¢„æµ‹ (æœªå½’ä¸€åŒ–çš„logits)
    'pred_logits': torch.Tensor,
    # shape: [batch_size, num_queries, num_classes]
    # ç¤ºä¾‹: [2, 300, 91]  (batch=2, 300ä¸ªæŸ¥è¯¢, 91ä¸ªCOCOç±»åˆ«)
    # å€¼åŸŸ: (-âˆ, +âˆ) æœªç»sigmoidæ¿€æ´»

    # è¾¹æ¡†é¢„æµ‹ (å½’ä¸€åŒ–çš„ç›¸å¯¹åæ ‡)
    'pred_boxes': torch.Tensor,
    # shape: [batch_size, num_queries, 4]
    # ç¤ºä¾‹: [2, 300, 4]
    # æ ¼å¼: [center_x, center_y, width, height]
    # å€¼åŸŸ: [0, 1] ç›¸å¯¹äºè¾“å…¥å›¾åƒå°ºå¯¸çš„å½’ä¸€åŒ–åæ ‡

    # åˆ†å‰²æ©ç é¢„æµ‹ (å¯é€‰, ä»…åœ¨segmentation_head=Trueæ—¶)
    'pred_masks': torch.Tensor,
    # shape: [batch_size, num_queries, H_mask, W_mask]
    # ç¤ºä¾‹: [2, 300, 160, 160]
    # å€¼åŸŸ: [0, 1] æ¦‚ç‡æ©ç 
}
```

**ç¤ºä¾‹æ•°æ®**:

```python
# å•ä¸ªæŸ¥è¯¢çš„åŸå§‹è¾“å‡º
{
    'pred_logits': tensor([[-2.5, 3.2, 1.8, ..., 0.5]]),  # 91ä¸ªç±»åˆ«çš„logits
    'pred_boxes': tensor([[0.5, 0.3, 0.15, 0.2]]),       # [cx, cy, w, h]
}

# ç»è¿‡sigmoidæ¿€æ´»å
prob = torch.sigmoid(pred_logits)  # [0.075, 0.961, 0.858, ..., 0.623]
max_score, max_class = prob.max()  # score=0.961, class=1 (person)
```

### 3.2 PostProcessåå¤„ç†è¾“å‡º

ç»è¿‡`PostProcess`æ¨¡å—å¤„ç†åï¼Œæ•°æ®è½¬æ¢ä¸ºCOCO APIå…¼å®¹æ ¼å¼ï¼š

```python
# æ–‡ä»¶: rfdetr/models/lwdetr.py - class PostProcess
postprocessed_results = [
    # æ¯ä¸ªbatchçš„è¾“å‡ºæ˜¯ä¸€ä¸ªå­—å…¸
    {
        # Top-Kç½®ä¿¡åº¦åˆ†æ•° (å·²sigmoidæ¿€æ´»)
        'scores': torch.Tensor,
        # shape: [num_select]  (é»˜è®¤100)
        # ç¤ºä¾‹: tensor([0.961, 0.885, 0.742, ...])  (é™åºæ’åˆ—)
        # å€¼åŸŸ: [0, 1]

        # é¢„æµ‹ç±»åˆ«ID (0-basedç´¢å¼•)
        'labels': torch.Tensor,
        # shape: [num_select]
        # ç¤ºä¾‹: tensor([1, 18, 3, ...])  (1=person, 18=dog, 3=car)
        # å€¼åŸŸ: [0, num_classes-1]

        # è¾¹æ¡†ç»å¯¹åæ ‡ (xyxyæ ¼å¼)
        'boxes': torch.Tensor,
        # shape: [num_select, 4]
        # æ ¼å¼: [x_min, y_min, x_max, y_max]
        # ç¤ºä¾‹: tensor([[272.0, 96.0, 368.0, 192.0], ...])
        # å€¼åŸŸ: ç»å¯¹åƒç´ åæ ‡ (åŸºäºåŸå›¾å°ºå¯¸)

        # åˆ†å‰²æ©ç  (å¯é€‰)
        'masks': torch.Tensor,
        # shape: [num_select, 1, orig_h, orig_w]
        # å¸ƒå°”æ©ç , å·²resizeåˆ°åŸå›¾å°ºå¯¸
    },
    # ... æ¯ä¸ªbatchçš„ç»“æœ
]
```

### 3.3 å…³é”®è½¬æ¢æ­¥éª¤

ä»¥ä¸‹æ˜¯`PostProcess.forward()`çš„è¯¦ç»†è½¬æ¢é€»è¾‘ï¼š

```python
# æ–‡ä»¶: rfdetr/models/lwdetr.py: PostProcess.forward() ç¬¬441-493è¡Œ

@torch.no_grad()
def forward(self, outputs, target_sizes):
    """
    å°†æ¨¡å‹è¾“å‡ºè½¬æ¢ä¸ºCOCOæ ¼å¼

    Args:
        outputs: æ¨¡å‹åŸå§‹è¾“å‡º
            - pred_logits: [B, Q, C] åˆ†ç±»logits
            - pred_boxes: [B, Q, 4] è¾¹æ¡† [cx, cy, w, h] (å½’ä¸€åŒ–)
        target_sizes: [B, 2] ç›®æ ‡å›¾åƒå°ºå¯¸ (height, width)

    Returns:
        results: List[Dict] æ¯ä¸ªå…ƒç´ åŒ…å« {'scores', 'labels', 'boxes'}
    """
    out_logits, out_bbox = outputs['pred_logits'], outputs['pred_boxes']

    # ========== æ­¥éª¤1: Sigmoidæ¿€æ´» + Top-Ké€‰æ‹© ==========
    prob = out_logits.sigmoid()  # [B, Q, C] -> æ¦‚ç‡åˆ†å¸ƒ
    topk_values, topk_indexes = torch.topk(
        prob.view(out_logits.shape[0], -1),  # å±•å¹³ä¸º [B, Q*C]
        self.num_select,  # é»˜è®¤100
        dim=1
    )
    # topk_values: [B, 100] Top-Kåˆ†æ•°
    # topk_indexes: [B, 100] åœ¨å±•å¹³ç»´åº¦çš„ç´¢å¼•

    scores = topk_values
    topk_boxes = topk_indexes // out_logits.shape[2]  # æŸ¥è¯¢ç´¢å¼•
    labels = topk_indexes % out_logits.shape[2]       # ç±»åˆ«ç´¢å¼•

    # ========== æ­¥éª¤2: è¾¹æ¡†æ ¼å¼è½¬æ¢ cxcywh -> xyxy ==========
    boxes = box_ops.box_cxcywh_to_xyxy(out_bbox)  # [B, Q, 4]
    # å…¬å¼:
    # x1 = cx - w/2
    # y1 = cy - h/2
    # x2 = cx + w/2
    # y2 = cy + h/2

    # æ ¹æ®Top-Kç´¢å¼•é€‰æ‹©å¯¹åº”çš„è¾¹æ¡†
    boxes = torch.gather(
        boxes, 1,
        topk_boxes.unsqueeze(-1).repeat(1, 1, 4)
    )  # [B, 100, 4]

    # ========== æ­¥éª¤3: ç¼©æ”¾åˆ°ç»å¯¹åæ ‡ ==========
    img_h, img_w = target_sizes.unbind(1)  # [B] é«˜åº¦å’Œå®½åº¦
    scale_fct = torch.stack([img_w, img_h, img_w, img_h], dim=1)  # [B, 4]
    boxes = boxes * scale_fct[:, None, :]  # å¹¿æ’­ä¹˜æ³•
    # ç¤ºä¾‹: [0.425, 0.2, 0.575, 0.4] * [640, 480, 640, 480]
    #    -> [272.0, 96.0, 368.0, 192.0]

    # ========== æ­¥éª¤4: ç»„è£…ç»“æœ ==========
    results = [
        {'scores': s, 'labels': l, 'boxes': b}
        for s, l, b in zip(scores, labels, boxes)
    ]

    return results
```

**æ•°æ®è½¬æ¢ç¤ºä¾‹**:

```python
# åŸå§‹è¾“å…¥
pred_boxes = torch.tensor([[0.5, 0.3, 0.15, 0.2]])  # [cx, cy, w, h] å½’ä¸€åŒ–
target_size = (480, 640)  # [height, width]

# æ­¥éª¤1: cxcywh -> xyxy (å½’ä¸€åŒ–)
x1 = 0.5 - 0.15/2 = 0.425
y1 = 0.3 - 0.2/2 = 0.2
x2 = 0.5 + 0.15/2 = 0.575
y2 = 0.3 + 0.2/2 = 0.4
xyxy_norm = [0.425, 0.2, 0.575, 0.4]

# æ­¥éª¤2: ç¼©æ”¾åˆ°ç»å¯¹åæ ‡
x1_abs = 0.425 * 640 = 272.0
y1_abs = 0.2 * 480 = 96.0
x2_abs = 0.575 * 640 = 368.0
y2_abs = 0.4 * 480 = 192.0
xyxy_abs = [272.0, 96.0, 368.0, 192.0]

# æœ€ç»ˆè¾“å‡º
result = {
    'scores': torch.tensor([0.961]),
    'labels': torch.tensor([1]),  # personç±»
    'boxes': torch.tensor([[272.0, 96.0, 368.0, 192.0]])
}
```

---

## 4. æ•°æ®æ¡¥æ¥åˆ°COCO API

### 4.1 å®Œæ•´æ¡¥æ¥æµç¨‹

```mermaid
graph TD
    A["æ¨¡å‹è¾“å‡º<br/>pred_logits, pred_boxes"] -->|PostProcess| B["åå¤„ç†ç»“æœ<br/>scores, labels, boxes"]
    B -->|evaluateå‡½æ•°| C["å­—å…¸æ ¼å¼<br/>{image_id: prediction}"]
    C -->|CocoEvaluator.update| D["prepare_for_coco_detection"]
    D -->|æ ¼å¼è½¬æ¢| E["COCO JSONæ ¼å¼<br/>[{image_id, category_id, bbox, score}]"]
    E -->|COCO.loadRes| F["pycocotools.COCOå¯¹è±¡"]
    F -->|COCOeval| G["è¯„ä¼°å¼•æ“æ‰§è¡Œ"]
    G -->|evaluate/accumulate/summarize| H["COCOæ ‡å‡†æŒ‡æ ‡<br/>12ä¸ªAP/ARæŒ‡æ ‡"]
    H -->|coco_extended_metrics| I["æ‰©å±•æŒ‡æ ‡<br/>Precision/Recall/F1"]

    style A fill:#e1f5ff
    style E fill:#fff4e1
    style H fill:#e8f5e9
    style I fill:#f3e5f5
```

### 4.2 æ­¥éª¤1: ç»„ç»‡ä¸ºå­—å…¸æ ¼å¼

```python
# æ–‡ä»¶: rfdetr/engine.py: evaluate() ç¬¬312-319è¡Œ

# æå–ç›®æ ‡å›¾åƒçš„åŸå§‹å°ºå¯¸
orig_target_sizes = torch.stack(
    [t["orig_size"] for t in targets],
    dim=0
)
# ç¤ºä¾‹: tensor([[480, 640], [600, 800]])  (height, width)

# è°ƒç”¨PostProcessè¿›è¡Œåå¤„ç†
results_all = postprocess(outputs, orig_target_sizes)
# è¿”å›: [
#     {'scores': Tensor[100], 'labels': Tensor[100], 'boxes': Tensor[100, 4]},
#     {'scores': Tensor[100], 'labels': Tensor[100], 'boxes': Tensor[100, 4]}
# ]

# ç»„ç»‡ä¸º {image_id: prediction} å­—å…¸æ ¼å¼
res = {
    target["image_id"].item(): output
    for target, output in zip(targets, results_all)
}
# ç¤ºä¾‹:
# res = {
#     139: {'scores': tensor([0.961, ...]), 'labels': tensor([1, ...]), 'boxes': tensor([[272.0, ...], ...])},
#     285: {'scores': tensor([0.885, ...]), 'labels': tensor([18, ...]), 'boxes': tensor([[150.2, ...], ...])},
#     ...
# }

# æ›´æ–°è¯„ä¼°å™¨
coco_evaluator.update(res)
```

### 4.3 æ­¥éª¤2: è½¬æ¢ä¸ºCOCOæ ‡å‡†JSONæ ¼å¼

```python
# æ–‡ä»¶: rfdetr/datasets/coco_eval.py: prepare_for_coco_detection() ç¬¬93-115è¡Œ

def prepare_for_coco_detection(self, predictions):
    """
    å°†é¢„æµ‹ç»“æœè½¬æ¢ä¸ºCOCOæ ‡å‡†æ ¼å¼

    Args:
        predictions: Dict[int, Dict[str, Tensor]]
            {image_id: {'boxes': Tensor[N,4], 'scores': Tensor[N], 'labels': Tensor[N]}}

    Returns:
        coco_results: List[Dict] COCOæ ¼å¼çš„æ£€æµ‹ç»“æœåˆ—è¡¨
            [{'image_id', 'category_id', 'bbox', 'score'}, ...]
    """
    coco_results = []

    for original_id, prediction in predictions.items():
        if len(prediction) == 0:
            continue

        # æå–é¢„æµ‹æ•°æ®
        boxes = prediction["boxes"]  # [N, 4] xyxyæ ¼å¼
        boxes = convert_to_xywh(boxes).tolist()  # è½¬æ¢ä¸ºxywhæ ¼å¼
        scores = prediction["scores"].tolist()
        labels = prediction["labels"].tolist()

        # è½¬æ¢ä¸ºCOCOæ ¼å¼
        coco_results.extend([
            {
                "image_id": original_id,      # å›¾åƒID (int)
                "category_id": labels[k],     # ç±»åˆ«ID (1-based, int)
                "bbox": box,                  # è¾¹æ¡† [x, y, w, h] (List[float])
                "score": scores[k],           # ç½®ä¿¡åº¦åˆ†æ•° (float)
            }
            for k, box in enumerate(boxes)
        ])

    return coco_results
```

**è¾¹æ¡†æ ¼å¼è½¬æ¢å‡½æ•°**:

```python
# æ–‡ä»¶: rfdetr/datasets/coco_eval.py: convert_to_xywh() ç¬¬179-181è¡Œ

def convert_to_xywh(boxes):
    """
    å°†è¾¹æ¡†ä»xyxyæ ¼å¼è½¬æ¢ä¸ºxywhæ ¼å¼

    Args:
        boxes: Tensor[N, 4] æ ¼å¼ä¸º [x_min, y_min, x_max, y_max]

    Returns:
        Tensor[N, 4] æ ¼å¼ä¸º [x, y, width, height]
    """
    xmin, ymin, xmax, ymax = boxes.unbind(1)
    return torch.stack(
        (xmin, ymin, xmax - xmin, ymax - ymin),
        dim=1
    )

# ç¤ºä¾‹:
# è¾“å…¥: tensor([[272.0, 96.0, 368.0, 192.0]])
# è¾“å‡º: tensor([[272.0, 96.0, 96.0, 96.0]])
```

**è½¬æ¢ç¤ºä¾‹**:

```python
# è¾“å…¥: PostProcessè¾“å‡º
prediction = {
    139: {
        'scores': tensor([0.961, 0.885]),
        'labels': tensor([1, 18]),
        'boxes': tensor([[272.0, 96.0, 368.0, 192.0],   # xyxyæ ¼å¼
                        [150.2, 200.5, 200.4, 280.8]])
    }
}

# æ­¥éª¤1: xyxy -> xywh
boxes_xywh = convert_to_xywh(prediction[139]['boxes'])
# tensor([[272.0, 96.0, 96.0, 96.0],
#         [150.2, 200.5, 50.2, 80.3]])

# æ­¥éª¤2: ç»„è£…COCOæ ¼å¼
coco_results = [
    {
        "image_id": 139,
        "category_id": 1,           # person
        "bbox": [272.0, 96.0, 96.0, 96.0],
        "score": 0.961
    },
    {
        "image_id": 139,
        "category_id": 18,          # dog
        "bbox": [150.2, 200.5, 50.2, 80.3],
        "score": 0.885
    }
]
```

### 4.4 æ­¥éª¤3: åŠ è½½åˆ°pycocotoolså¹¶è¯„ä¼°

```python
# æ–‡ä»¶: rfdetr/datasets/coco_eval.py: update() ç¬¬50-67è¡Œ

def update(self, predictions):
    """
    æ›´æ–°è¯„ä¼°ç»“æœ

    Args:
        predictions: Dict[int, Dict[str, Tensor]] é¢„æµ‹ç»“æœå­—å…¸
    """
    # æå–æ‰€æœ‰å›¾åƒID
    img_ids = list(np.unique(list(predictions.keys())))
    self.img_ids.extend(img_ids)

    for iou_type in self.iou_types:  # ['bbox'] æˆ– ['bbox', 'segm']
        # å‡†å¤‡COCOæ ¼å¼çš„ç»“æœ
        results = self.prepare(predictions, iou_type)
        # results = [{'image_id', 'category_id', 'bbox', 'score'}, ...]

        # åŠ è½½é¢„æµ‹ç»“æœåˆ°COCO API (æŠ‘åˆ¶è¾“å‡º)
        with open(os.devnull, 'w') as devnull:
            with contextlib.redirect_stdout(devnull):
                coco_dt = COCO.loadRes(self.coco_gt, results) if results else COCO()

        # åˆ›å»ºè¯„ä¼°å¯¹è±¡
        coco_eval = self.coco_eval[iou_type]
        coco_eval.cocoDt = coco_dt  # é¢„æµ‹ç»“æœ
        coco_eval.params.imgIds = list(img_ids)

        # æ‰§è¡Œè¯„ä¼° (è°ƒç”¨è‡ªå®šä¹‰çš„evaluateå‡½æ•°)
        img_ids, eval_imgs = evaluate(coco_eval)
        self.eval_imgs[iou_type].append(eval_imgs)
```

**è‡ªå®šä¹‰evaluateå‡½æ•°** (æ”¯æŒåˆ†å¸ƒå¼è®­ç»ƒ):

```python
# æ–‡ä»¶: rfdetr/datasets/coco_eval.py: evaluate() ç¬¬184-228è¡Œ

def evaluate(imgs):
    """
    è‡ªå®šä¹‰è¯„ä¼°å‡½æ•°ï¼Œæ”¯æŒåˆ†å¸ƒå¼å¹¶è¡Œ

    Args:
        imgs: COCOevalå¯¹è±¡ï¼ŒåŒ…å«GTå’Œé¢„æµ‹ç»“æœ

    Returns:
        img_ids: å›¾åƒIDåˆ—è¡¨
        eval_imgs: è¯„ä¼°ç»“æœåˆ—è¡¨ (æ¯ä¸ªå›¾åƒçš„åŒ¹é…ç»“æœ)
    """
    if hasattr(imgs, 'params'):
        # æ ‡å‡†COCOevalå¯¹è±¡
        p = imgs.params
        catIds = p.catIds if p.useCats else [-1]

        # è°ƒç”¨pycocotoolsçš„computeIoUå‡½æ•°
        imgs.computeIoU = computeIoU

        # è¯„ä¼°æ¯ä¸ªç±»åˆ«ã€æ¯ä¸ªåŒºåŸŸã€æ¯ä¸ªå›¾åƒ
        imgs.ious = {
            (imgId, catId): imgs.computeIoU(imgId, catId)
            for imgId in p.imgIds
            for catId in catIds
        }

        # å¹¶è¡Œè¯„ä¼°æ‰€æœ‰å›¾åƒ
        evaluateImg = imgs.evaluateImg
        maxDet = p.maxDets[-1]
        evalImgs = [
            evaluateImg(imgId, catId, areaRng, maxDet)
            for catId in catIds
            for areaRng in p.areaRng
            for imgId in p.imgIds
        ]

        # é‡ç»„ä¸º[T, R, K, A, M]æ ¼å¼
        evalImgs = np.asarray(evalImgs).reshape(len(catIds), len(p.areaRng), len(p.imgIds))
        imgs._paramsEval = copy.deepcopy(imgs.params)

        return p.imgIds, evalImgs
```

### 4.5 æ­¥éª¤4: ç´¯ç§¯å’Œæ±‡æ€»æŒ‡æ ‡

```python
# æ–‡ä»¶: rfdetr/datasets/coco_eval.py: accumulate() & summarize()

def accumulate(self):
    """ç´¯ç§¯æ‰€æœ‰å›¾åƒçš„è¯„ä¼°ç»“æœ"""
    for coco_eval in self.coco_eval.values():
        coco_eval.eval = self.merge(coco_eval)
        coco_eval.accumulate()  # è°ƒç”¨pycocotoolsçš„ç´¯ç§¯å‡½æ•°

def summarize(self):
    """æ‰“å°è¯„ä¼°æ‘˜è¦"""
    for iou_type, coco_eval in self.coco_eval.items():
        print("IoU metric: {}".format(iou_type))
        coco_eval.summarize()  # è°ƒç”¨pycocotoolsçš„æ±‡æ€»å‡½æ•°
        # è¾“å‡ºç¤ºä¾‹:
        # Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.425
        # Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.648
        # ...
```

---

## 5. è¯„ä¼°æŒ‡æ ‡è¯¦è§£

### 5.1 æ ‡å‡†COCOæŒ‡æ ‡ (12ä¸ª)

æ¥æº: `pycocotools.cocoeval.COCOeval.summarize()`

```python
# æ‰€æœ‰æŒ‡æ ‡å­˜å‚¨åœ¨ coco_eval.stats æ•°ç»„ä¸­
coco_eval.stats = np.array([
    stats[0],   # AP @ IoU=0.50:0.95 (ä¸»æŒ‡æ ‡)
    stats[1],   # AP @ IoU=0.50
    stats[2],   # AP @ IoU=0.75
    stats[3],   # AP @ IoU=0.50:0.95 | area=small
    stats[4],   # AP @ IoU=0.50:0.95 | area=medium
    stats[5],   # AP @ IoU=0.50:0.95 | area=large
    stats[6],   # AR @ IoU=0.50:0.95 | maxDets=1
    stats[7],   # AR @ IoU=0.50:0.95 | maxDets=10
    stats[8],   # AR @ IoU=0.50:0.95 | maxDets=100
    stats[9],   # AR @ IoU=0.50:0.95 | area=small
    stats[10],  # AR @ IoU=0.50:0.95 | area=medium
    stats[11],  # AR @ IoU=0.50:0.95 | area=large
])
```

**æŒ‡æ ‡è¯¦ç»†è¯´æ˜**:

| æŒ‡æ ‡ç¼–å· | åç§° | è¯´æ˜ | IoUé˜ˆå€¼ | åŒºåŸŸèŒƒå›´ | æœ€å¤§æ£€æµ‹æ•° |
|---------|------|------|---------|---------|-----------|
| **stats[0]** | **mAP** | å¹³å‡ç²¾åº¦å‡å€¼ (ä¸»æŒ‡æ ‡) | 0.50:0.95 (æ­¥é•¿0.05) | all | 100 |
| stats[1] | AP50 | IoU=0.5æ—¶çš„AP | 0.50 | all | 100 |
| stats[2] | AP75 | IoU=0.75æ—¶çš„AP | 0.75 | all | 100 |
| stats[3] | AP_small | å°ç›®æ ‡çš„AP | 0.50:0.95 | < 32Â² | 100 |
| stats[4] | AP_medium | ä¸­ç›®æ ‡çš„AP | 0.50:0.95 | 32Â² ~ 96Â² | 100 |
| stats[5] | AP_large | å¤§ç›®æ ‡çš„AP | 0.50:0.95 | > 96Â² | 100 |
| stats[6] | AR_1 | æœ€å¤š1ä¸ªæ£€æµ‹çš„å¹³å‡å¬å› | 0.50:0.95 | all | **1** |
| stats[7] | AR_10 | æœ€å¤š10ä¸ªæ£€æµ‹çš„å¹³å‡å¬å› | 0.50:0.95 | all | **10** |
| stats[8] | AR_100 | æœ€å¤š100ä¸ªæ£€æµ‹çš„å¹³å‡å¬å› | 0.50:0.95 | all | **100** |
| stats[9] | AR_small | å°ç›®æ ‡çš„å¹³å‡å¬å› | 0.50:0.95 | < 32Â² | 100 |
| stats[10] | AR_medium | ä¸­ç›®æ ‡çš„å¹³å‡å¬å› | 0.50:0.95 | 32Â² ~ 96Â² | 100 |
| stats[11] | AR_large | å¤§ç›®æ ‡çš„å¹³å‡å¬å› | 0.50:0.95 | > 96Â² | 100 |

**å…³é”®æ¦‚å¿µ**:

- **IoUé˜ˆå€¼**: é¢„æµ‹æ¡†ä¸GTæ¡†çš„é‡å åº¦ï¼Œ0.50:0.95è¡¨ç¤ºä½¿ç”¨10ä¸ªé˜ˆå€¼(0.50, 0.55, ..., 0.95)çš„å¹³å‡å€¼
- **åŒºåŸŸèŒƒå›´**: åŸºäºè¾¹æ¡†é¢ç§¯åˆ’åˆ†
  - Small: é¢ç§¯ < 32Â² = 1024 åƒç´ Â²
  - Medium: 1024 â‰¤ é¢ç§¯ < 96Â² = 9216 åƒç´ Â²
  - Large: é¢ç§¯ â‰¥ 9216 åƒç´ Â²
- **maxDets**: æ¯å¼ å›¾åƒæœ€å¤šè€ƒè™‘çš„æ£€æµ‹æ¡†æ•°é‡

### 5.2 RF-DETRæ‰©å±•æŒ‡æ ‡

æ¥æº: `rfdetr/engine.py: coco_extended_metrics()` ç¬¬181-250è¡Œ

```python
# æ‰©å±•æŒ‡æ ‡è¿”å›ä¸€ä¸ªå­—å…¸
extended_metrics = {
    # ========== å…¨å±€æŒ‡æ ‡ ==========
    "map@50:95": float,        # ç­‰åŒäº coco_eval.stats[0]
    "map@50": float,           # ç­‰åŒäº coco_eval.stats[1]
    "precision": float,        # å®å¹³å‡ç²¾ç¡®ç‡ (åœ¨æœ€ä½³F1ç‚¹)
    "recall": float,           # å®å¹³å‡å¬å›ç‡ (åœ¨æœ€ä½³F1ç‚¹)
    "f1": float,               # å®å¹³å‡F1åˆ†æ•° (æœ€å¤§å€¼)

    # ========== æ¯ä¸ªç±»åˆ«çš„è¯¦ç»†æŒ‡æ ‡ ==========
    "class_map": [
        # æ¯ä¸ªç±»åˆ«çš„æŒ‡æ ‡
        {
            "class": str,           # ç±»åˆ«åç§° (å¦‚ "person", "car")
            "map@50:95": float,     # è¯¥ç±»çš„AP@0.5:0.95
            "map@50": float,        # è¯¥ç±»çš„AP@50
            "precision": float,     # è¯¥ç±»çš„ç²¾ç¡®ç‡ (åœ¨æœ€ä½³F1ç‚¹)
            "recall": float,        # è¯¥ç±»çš„å¬å›ç‡ (åœ¨æœ€ä½³F1ç‚¹)
            "f1": float,            # è¯¥ç±»çš„F1åˆ†æ•°
        },
        # ... 80ä¸ªCOCOç±»åˆ«

        # æ‰€æœ‰ç±»åˆ«çš„æ±‡æ€» (macro average)
        {
            "class": "all",
            "map@50:95": float,     # æ‰€æœ‰ç±»åˆ«çš„å®å¹³å‡AP
            "map@50": float,
            "precision": float,
            "recall": float,
            "f1": float
        }
    ]
}
```

**æ‰©å±•æŒ‡æ ‡è®¡ç®—é€»è¾‘**:

```python
# æ–‡ä»¶: rfdetr/engine.py: coco_extended_metrics() ç¬¬181-250è¡Œ

def coco_extended_metrics(coco_eval, coco=None):
    """
    è®¡ç®—æ‰©å±•çš„COCOæŒ‡æ ‡ï¼ŒåŒ…æ‹¬ç²¾ç¡®ç‡ã€å¬å›ç‡ã€F1åˆ†æ•°

    Args:
        coco_eval: COCOevalå¯¹è±¡ (å·²è°ƒç”¨accumulate)
        coco: COCOå¯¹è±¡ (ground truth)

    Returns:
        results: Dict åŒ…å«å…¨å±€å’Œæ¯ç±»åˆ«æŒ‡æ ‡
    """
    # ========== æå–ç²¾ç¡®ç‡çŸ©é˜µ ==========
    iou_thrs = coco_eval.params.iouThrs  # [0.50, 0.55, ..., 0.95]
    rec_thrs = coco_eval.params.recThrs  # [0.0, 0.01, 0.02, ..., 1.0]

    # æ‰¾åˆ°IoU=0.5çš„ç´¢å¼•
    iou50_idx = int(np.argwhere(np.isclose(iou_thrs, 0.50)))

    # æå–ç²¾ç¡®ç‡æ•°æ®
    # Pçš„shape: [T, R, K, A, M]
    # T: IoUé˜ˆå€¼æ•° (10), R: å¬å›é˜ˆå€¼æ•° (101), K: ç±»åˆ«æ•° (80)
    # A: åŒºåŸŸèŒƒå›´æ•° (4), M: æœ€å¤§æ£€æµ‹æ•° (3)
    P = coco_eval.eval["precision"]

    # æå–IoU=0.5ã€area=allã€maxDets=100çš„ç²¾ç¡®ç‡
    prec_raw = P[iou50_idx, :, :, area_idx, maxdet_idx]  # [101, 80]

    # ========== è®¡ç®—F1åˆ†æ•°å¹¶æ‰¾åˆ°æœ€ä½³é˜ˆå€¼ ==========
    prec = prec_raw.copy().astype(float)
    prec[prec < 0] = np.nan  # å°†-1æ›¿æ¢ä¸ºNaN

    # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„F1åˆ†æ•°
    # F1 = 2 * P * R / (P + R)
    f1_cls = 2 * prec * rec_thrs[:, None] / (prec + rec_thrs[:, None] + 1e-8)
    # f1_cls shape: [101, 80]

    # å®å¹³å‡F1 (æ‰€æœ‰ç±»åˆ«å¹³å‡)
    f1_macro = np.nanmean(f1_cls, axis=1)  # [101]

    # æ‰¾åˆ°æœ€ä½³F1é˜ˆå€¼
    best_j = int(f1_macro.argmax())

    # åœ¨æœ€ä½³é˜ˆå€¼ç‚¹è®¡ç®—å…¨å±€æŒ‡æ ‡
    macro_precision = float(np.nanmean(prec[best_j]))
    macro_recall = float(rec_thrs[best_j])
    macro_f1 = float(f1_macro[best_j])

    # ========== è®¡ç®—æ¯ä¸ªç±»åˆ«çš„æŒ‡æ ‡ ==========
    class_map = []
    cat_ids = coco_eval.params.catIds

    for k, cid in enumerate(cat_ids):
        # æå–è¯¥ç±»åˆ«çš„ç²¾ç¡®ç‡çŸ©é˜µ
        p_slice = P[:, :, k, area_idx, maxdet_idx]  # [T, R]

        # AP@0.50:0.95 (æ‰€æœ‰IoUé˜ˆå€¼çš„å¹³å‡)
        valid = (p_slice > -1).any(axis=1)
        ap_50_95 = float(p_slice[valid].mean()) if valid.any() else float('nan')

        # AP@0.50
        ap_50 = float(p_slice[iou50_idx].mean()) if (p_slice[iou50_idx] > -1).any() else float('nan')

        # ç²¾ç¡®ç‡ã€å¬å›ç‡ã€F1 (åœ¨æœ€ä½³é˜ˆå€¼ç‚¹)
        precision_cls = float(prec[best_j, k]) if not np.isnan(prec[best_j, k]) else float('nan')
        recall_cls = float(rec_thrs[best_j])
        f1_cls_val = float(f1_cls[best_j, k]) if not np.isnan(f1_cls[best_j, k]) else float('nan')

        # è·å–ç±»åˆ«åç§°
        class_name = coco.cats[cid]['name'] if coco else str(cid)

        class_map.append({
            "class": class_name,
            "map@50:95": ap_50_95,
            "map@50": ap_50,
            "precision": precision_cls,
            "recall": recall_cls,
            "f1": f1_cls_val
        })

    # æ·»åŠ "all"ç±»åˆ«çš„æ±‡æ€»
    class_map.append({
        "class": "all",
        "map@50:95": float(coco_eval.stats[0]),
        "map@50": float(coco_eval.stats[1]),
        "precision": macro_precision,
        "recall": macro_recall,
        "f1": macro_f1
    })

    # ========== ç»„è£…æœ€ç»ˆç»“æœ ==========
    results = {
        "map@50:95": float(coco_eval.stats[0]),
        "map@50": float(coco_eval.stats[1]),
        "precision": macro_precision,
        "recall": macro_recall,
        "f1": macro_f1,
        "class_map": class_map
    }

    return results
```

**ä¸ºä»€ä¹ˆéœ€è¦æ‰©å±•æŒ‡æ ‡ï¼Ÿ**

1. **æ ‡å‡†COCOæŒ‡æ ‡çš„å±€é™æ€§**:
   - åªæä¾›APå’ŒARï¼Œç¼ºå°‘ç²¾ç¡®ç‡å’Œå¬å›ç‡
   - æ²¡æœ‰F1åˆ†æ•°ï¼Œéš¾ä»¥è¯„ä¼°ç²¾ç¡®ç‡-å¬å›ç‡çš„å¹³è¡¡
   - æ— æ³•å¿«é€Ÿå®šä½å“ªäº›ç±»åˆ«è¡¨ç°ä¸ä½³

2. **RF-DETRæ‰©å±•æŒ‡æ ‡çš„ä¼˜åŠ¿**:
   - **Precision/Recall/F1**: æä¾›æ›´ç›´è§‚çš„æ€§èƒ½æŒ‡æ ‡
   - **è‡ªåŠ¨é˜ˆå€¼é€‰æ‹©**: åœ¨æœ€ä½³F1ç‚¹è®¡ç®—æŒ‡æ ‡ï¼Œé¿å…æ‰‹åŠ¨è°ƒå‚
   - **æŒ‰ç±»åˆ«ç»†åˆ†**: å¿«é€Ÿè¯†åˆ«è¡¨ç°å·®çš„ç±»åˆ«
   - **JSONå¯¼å‡º**: ä¾¿äºç¨‹åºåŒ–åˆ†æå’Œå¯è§†åŒ–

### 5.3 æŒ‡æ ‡è¾“å‡ºç¤ºä¾‹

```python
# æ ‡å‡†COCOè¾“å‡º (è°ƒç”¨coco_eval.summarize())
"""
IoU metric: bbox
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.425
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.648
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.456
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.218
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.465
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.612
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.321
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.512
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.547
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.325
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.592
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.721
"""

# RF-DETRæ‰©å±•æŒ‡æ ‡è¾“å‡º (JSONæ ¼å¼)
{
    "map@50:95": 0.425,
    "map@50": 0.648,
    "precision": 0.512,
    "recall": 0.687,
    "f1": 0.587,
    "class_map": [
        {
            "class": "person",
            "map@50:95": 0.558,
            "map@50": 0.742,
            "precision": 0.625,
            "recall": 0.687,
            "f1": 0.654
        },
        {
            "class": "bicycle",
            "map@50:95": 0.312,
            "map@50": 0.521,
            "precision": 0.425,
            "recall": 0.687,
            "f1": 0.525
        },
        // ... 78ä¸ªå…¶ä»–ç±»åˆ«
        {
            "class": "all",
            "map@50:95": 0.425,
            "map@50": 0.648,
            "precision": 0.512,
            "recall": 0.687,
            "f1": 0.587
        }
    ]
}
```

---

## 6. å®Œæ•´ç«¯åˆ°ç«¯æµç¨‹ç¤ºä¾‹

### 6.1 è¯„ä¼°ä¸»å¾ªç¯ä»£ç 

```python
# æ–‡ä»¶: rfdetr/engine.py: evaluate() ç¬¬267-341è¡Œ

@torch.no_grad()
def evaluate(model, criterion, postprocess, data_loader, base_ds, device, args):
    """
    è¯„ä¼°æ¨¡å‹åœ¨æ•°æ®é›†ä¸Šçš„æ€§èƒ½

    Args:
        model: RF-DETRæ¨¡å‹
        criterion: æŸå¤±å‡½æ•° (SetCriterion)
        postprocess: åå¤„ç†æ¨¡å— (PostProcess)
        data_loader: æ•°æ®åŠ è½½å™¨
        base_ds: COCOæ•°æ®é›†å¯¹è±¡ (ç”¨äºground truth)
        device: è®¾å¤‡ (cuda/cpu)
        args: å‘½ä»¤è¡Œå‚æ•°

    Returns:
        stats: COCOè¯„ä¼°ç»Ÿè®¡ä¿¡æ¯
        coco_evaluator: CocoEvaluatorå¯¹è±¡
    """
    model.eval()
    criterion.eval()

    # ========== åˆå§‹åŒ–è¯„ä¼°å™¨ ==========
    iou_types = ("bbox",) if not args.segmentation_head else ("bbox", "segm")
    coco_evaluator = CocoEvaluator(base_ds, iou_types)

    # ========== æ¨ç†å¾ªç¯ ==========
    for samples, targets in data_loader:
        # 1. æ•°æ®ç§»åŠ¨åˆ°è®¾å¤‡
        samples = samples.to(device)
        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

        # 2. æ¨¡å‹å‰å‘æ¨ç†
        with autocast(**get_autocast_args(args)):
            outputs = model(samples)
            # outputs = {
            #     'pred_logits': Tensor[batch, 300, 91],
            #     'pred_boxes': Tensor[batch, 300, 4]
            # }

        # 3. è®¡ç®—æŸå¤± (å¯é€‰)
        loss_dict = criterion(outputs, targets)

        # 4. åå¤„ç†
        orig_target_sizes = torch.stack([t["orig_size"] for t in targets], dim=0)
        results_all = postprocess(outputs, orig_target_sizes)
        # results_all = [
        #     {'scores': Tensor[100], 'labels': Tensor[100], 'boxes': Tensor[100, 4]},
        #     ...
        # ]

        # 5. ç»„ç»‡ä¸ºå­—å…¸æ ¼å¼
        res = {
            target["image_id"].item(): output
            for target, output in zip(targets, results_all)
        }

        # 6. æ›´æ–°è¯„ä¼°å™¨
        coco_evaluator.update(res)

    # ========== åŒæ­¥åˆ†å¸ƒå¼è¿›ç¨‹ ==========
    coco_evaluator.synchronize_between_processes()

    # ========== ç´¯ç§¯å¹¶æ±‡æ€» ==========
    coco_evaluator.accumulate()
    coco_evaluator.summarize()

    # ========== è®¡ç®—æ‰©å±•æŒ‡æ ‡ ==========
    results_json = coco_extended_metrics(
        coco_evaluator.coco_eval["bbox"],
        coco=base_ds if hasattr(base_ds, 'cats') else None
    )

    # ========== ä¿å­˜ç»“æœ ==========
    if args.output_dir:
        with open(args.output_dir / "results.json", "w") as f:
            json.dump(results_json, f, indent=4)

    return coco_evaluator.coco_eval["bbox"].stats, coco_evaluator
```

### 6.2 æ•°æ®æµå¯è§†åŒ–

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Step 1: æ•°æ®åŠ è½½                       â”‚
â”‚   [batch_imgs, batch_targets]           â”‚
â”‚   - imgs: Tensor[B,3,H,W]               â”‚
â”‚   - targets: List[Dict]                 â”‚
â”‚     {image_id, orig_size, labels, boxes}â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Step 2: æ¨¡å‹æ¨ç†                       â”‚
â”‚   outputs = model(imgs)                 â”‚
â”‚   - pred_logits: [B,300,91]             â”‚
â”‚   - pred_boxes: [B,300,4] (cxcywh, 0-1) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Step 3: PostProcessåå¤„ç†              â”‚
â”‚   results = postprocess(outputs, sizes)  â”‚
â”‚   - scores: [100] (Top-K)               â”‚
â”‚   - labels: [100]                       â”‚
â”‚   - boxes: [100,4] (xyxy, ç»å¯¹åæ ‡)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Step 4: ç»„ç»‡å­—å…¸æ ¼å¼                    â”‚
â”‚   res = {image_id: prediction}          â”‚
â”‚   {139: {'scores': ..., 'labels': ...}} â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Step 5: è½¬æ¢ä¸ºCOCOæ ¼å¼                  â”‚
â”‚   coco_results = [                      â”‚
â”‚     {"image_id": 139,                   â”‚
â”‚      "category_id": 1,                  â”‚
â”‚      "bbox": [272,96,96,96], (xywh)     â”‚
â”‚      "score": 0.961}                    â”‚
â”‚   ]                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Step 6: åŠ è½½åˆ°COCO API                 â”‚
â”‚   coco_dt = COCO.loadRes(coco_gt, res)  â”‚
â”‚   coco_eval = COCOeval(coco_gt, coco_dt)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Step 7: æ‰§è¡Œè¯„ä¼°                       â”‚
â”‚   coco_eval.evaluate()                  â”‚
â”‚   coco_eval.accumulate()                â”‚
â”‚   coco_eval.summarize()                 â”‚
â”‚   -> 12ä¸ªæ ‡å‡†COCOæŒ‡æ ‡                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Step 8: è®¡ç®—æ‰©å±•æŒ‡æ ‡                    â”‚
â”‚   results_json = coco_extended_metrics() â”‚
â”‚   -> Precision/Recall/F1 + Per-Class    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 6.3 å…·ä½“æ•°æ®è½¬æ¢ç¤ºä¾‹

å‡è®¾æ¨¡å‹å¯¹ä¸€å¼ å›¾åƒçš„æ¨ç†ç»“æœå¦‚ä¸‹ï¼š

```python
# ========== åŸå§‹æ¨¡å‹è¾“å‡º (Step 2) ==========
outputs = {
    'pred_logits': torch.tensor([[
        [-2.5, 3.2, 1.8, ..., 0.5],  # query 0: 91ä¸ªç±»åˆ«çš„logits
        [0.8, -1.2, 2.5, ..., 1.1],  # query 1
        # ... 298ä¸ªå…¶ä»–æŸ¥è¯¢
    ]]),  # shape: [1, 300, 91]

    'pred_boxes': torch.tensor([[
        [0.5, 0.3, 0.15, 0.2],  # query 0: [cx, cy, w, h] å½’ä¸€åŒ–
        [0.2, 0.6, 0.1, 0.15],  # query 1
        # ... 298ä¸ªå…¶ä»–æŸ¥è¯¢
    ]])   # shape: [1, 300, 4]
}
target_sizes = torch.tensor([[480, 640]])  # [height, width]

# ========== PostProcessåå¤„ç† (Step 3) ==========
# 3.1 Sigmoid + Top-K
prob = outputs['pred_logits'].sigmoid()  # [1, 300, 91]
# prob[0, 0] = [0.075, 0.961, 0.858, ..., 0.623]
# prob[0, 1] = [0.689, 0.231, 0.924, ..., 0.751]

topk_values, topk_indexes = torch.topk(prob.view(1, -1), 100, dim=1)
# topk_values[0] = [0.961, 0.924, 0.858, ...]  (é™åº)
# topk_indexes[0] = [1, 27303, 2, ...]  (åœ¨27300ä¸ªå…ƒç´ ä¸­çš„ç´¢å¼•)

# è§£æç´¢å¼•
scores = topk_values[0]  # [100]
topk_boxes = topk_indexes[0] // 91  # æŸ¥è¯¢ç´¢å¼•: [0, 300, 0, ...]
labels = topk_indexes[0] % 91        # ç±»åˆ«ç´¢å¼•: [1, 18, 2, ...]

# 3.2 è¾¹æ¡†è½¬æ¢ cxcywh -> xyxy
boxes_norm = box_cxcywh_to_xyxy(outputs['pred_boxes'])
# è¾“å…¥: [[0.5, 0.3, 0.15, 0.2]]
# è¾“å‡º: [[0.425, 0.2, 0.575, 0.4]]

# é€‰æ‹©Top-Kå¯¹åº”çš„è¾¹æ¡†
boxes_norm_topk = boxes_norm[0, topk_boxes]  # [100, 4]

# 3.3 ç¼©æ”¾åˆ°ç»å¯¹åæ ‡
scale_fct = torch.tensor([640, 480, 640, 480])  # [w, h, w, h]
boxes_abs = boxes_norm_topk * scale_fct
# boxes_abs[0] = [272.0, 96.0, 368.0, 192.0]  (xyxyæ ¼å¼)

# ç»„è£…ç»“æœ
result = {
    'scores': scores,   # [0.961, 0.924, ...]
    'labels': labels,   # [1, 18, 2, ...]
    'boxes': boxes_abs  # [[272.0, 96.0, 368.0, 192.0], ...]
}

# ========== ç»„ç»‡å­—å…¸æ ¼å¼ (Step 4) ==========
res = {
    139: result  # image_id=139
}

# ========== è½¬æ¢ä¸ºCOCOæ ¼å¼ (Step 5) ==========
# 5.1 xyxy -> xywh
boxes_xywh = convert_to_xywh(result['boxes'])
# è¾“å…¥: [[272.0, 96.0, 368.0, 192.0]]
# è¾“å‡º: [[272.0, 96.0, 96.0, 96.0]]

# 5.2 ç»„è£…COCOæ ¼å¼
coco_results = [
    {
        "image_id": 139,
        "category_id": 1,           # label=1 -> person
        "bbox": [272.0, 96.0, 96.0, 96.0],
        "score": 0.961
    },
    {
        "image_id": 139,
        "category_id": 18,          # label=18 -> dog
        "bbox": [128.0, 288.0, 64.0, 72.0],
        "score": 0.924
    },
    # ... 98ä¸ªå…¶ä»–æ£€æµ‹
]

# ========== åŠ è½½åˆ°COCO API (Step 6) ==========
coco_dt = COCO.loadRes(coco_gt, coco_results)
# å†…éƒ¨å°†coco_resultsè½¬æ¢ä¸ºCOCOçš„æ•°æ®ç»“æ„

coco_eval = COCOeval(coco_gt, coco_dt, iouType='bbox')
# å‡†å¤‡è¯„ä¼°å¯¹è±¡

# ========== æ‰§è¡Œè¯„ä¼° (Step 7) ==========
coco_eval.evaluate()    # è®¡ç®—IoUå¹¶åŒ¹é…GT
coco_eval.accumulate()  # ç´¯ç§¯æ‰€æœ‰å›¾åƒçš„ç»“æœ
coco_eval.summarize()   # è®¡ç®—12ä¸ªæ ‡å‡†æŒ‡æ ‡

# è¾“å‡º:
# Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.425
# ...

# ========== è®¡ç®—æ‰©å±•æŒ‡æ ‡ (Step 8) ==========
results_json = coco_extended_metrics(coco_eval)
# {
#     "map@50:95": 0.425,
#     "precision": 0.512,
#     "recall": 0.687,
#     "f1": 0.587,
#     "class_map": [...]
# }
```

---

## 7. ä¸é¡¹ç›®ç°æœ‰ä»£ç çš„é›†æˆæ–¹æ¡ˆ

### 7.1 å¯å¤ç”¨ç»„ä»¶æ¸…å•

åŸºäºRF-DETRçš„å®ç°ï¼Œä»¥ä¸‹ç»„ä»¶å¯ä»¥ç›´æ¥è¿ç§»åˆ°ä½ çš„ `tools/eval.py` ä¸­ï¼š

| ç»„ä»¶åç§° | æºæ–‡ä»¶ | åŠŸèƒ½ | å»ºè®®ç”¨é€” |
|---------|--------|------|---------|
| **CocoEvaluator** | `rfdetr/datasets/coco_eval.py` | å®Œæ•´çš„COCOè¯„ä¼°æµç¨‹ | ä¸»è¯„ä¼°å™¨ |
| **prepare_for_coco_detection** | `rfdetr/datasets/coco_eval.py` | æ ¼å¼è½¬æ¢å‡½æ•° | æ•°æ®æ¡¥æ¥ |
| **convert_to_xywh** | `rfdetr/datasets/coco_eval.py` | è¾¹æ¡†æ ¼å¼è½¬æ¢ | è¾…åŠ©å‡½æ•° |
| **coco_extended_metrics** | `rfdetr/engine.py` | æ‰©å±•æŒ‡æ ‡è®¡ç®— | å¢å¼ºæŠ¥å‘Š |
| **evaluate** | `rfdetr/datasets/coco_eval.py` | è‡ªå®šä¹‰è¯„ä¼°å‡½æ•° | åˆ†å¸ƒå¼æ”¯æŒ |

### 7.2 é›†æˆåˆ° `tools/eval.py` çš„å»ºè®®

#### æ–¹æ¡ˆ1: ç›´æ¥å¯¼å…¥RF-DETRæ¨¡å—

```python
# tools/eval.py
import sys
sys.path.insert(0, './third_party/rfdetr')

from datasets.coco_eval import CocoEvaluator, convert_to_xywh
from engine import coco_extended_metrics

def evaluate_detector_on_coco(detector, dataloader, coco_gt, output_dir):
    """ä½¿ç”¨RF-DETRçš„è¯„ä¼°å™¨è¯„ä¼°æ£€æµ‹æ¨¡å‹"""

    # åˆå§‹åŒ–è¯„ä¼°å™¨
    coco_evaluator = CocoEvaluator(coco_gt, iou_types=['bbox'])

    for images, targets in dataloader:
        # æ¨¡å‹æ¨ç†
        outputs = detector(images)
        # outputsåº”åŒ…å«: {'boxes': Tensor[N,4], 'scores': Tensor[N], 'labels': Tensor[N]}

        # ç»„ç»‡ä¸ºè¯„ä¼°æ ¼å¼
        results = {
            target['image_id'].item(): {
                'boxes': outputs['boxes'][i],    # xyxyæ ¼å¼, ç»å¯¹åæ ‡
                'scores': outputs['scores'][i],
                'labels': outputs['labels'][i]
            }
            for i, target in enumerate(targets)
        }

        # æ›´æ–°è¯„ä¼°å™¨
        coco_evaluator.update(results)

    # ç´¯ç§¯å’Œæ±‡æ€»
    coco_evaluator.accumulate()
    coco_evaluator.summarize()

    # è®¡ç®—æ‰©å±•æŒ‡æ ‡
    metrics = coco_extended_metrics(
        coco_evaluator.coco_eval['bbox'],
        coco=coco_gt
    )

    # ä¿å­˜ç»“æœ
    with open(output_dir / "coco_metrics.json", "w") as f:
        json.dump(metrics, f, indent=4)

    return metrics
```

#### æ–¹æ¡ˆ2: å¤åˆ¶å…³é”®å‡½æ•°åˆ°é¡¹ç›®ä¸­

å¦‚æœä¸æƒ³ä¾èµ–RF-DETRæ¨¡å—ï¼Œå¯ä»¥å¤åˆ¶å…³é”®å‡½æ•°åˆ° `utils/coco_metrics.py`:

```python
# utils/coco_metrics.py

import numpy as np
import torch
from pycocotools.coco import COCO
from pycocotools.cocoeval import COCOeval

def convert_to_xywh(boxes):
    """
    å°†è¾¹æ¡†ä»xyxyæ ¼å¼è½¬æ¢ä¸ºxywhæ ¼å¼

    Args:
        boxes: Tensor[N, 4] æ ¼å¼ä¸º [x_min, y_min, x_max, y_max]

    Returns:
        Tensor[N, 4] æ ¼å¼ä¸º [x, y, width, height]
    """
    xmin, ymin, xmax, ymax = boxes.unbind(1)
    return torch.stack((xmin, ymin, xmax - xmin, ymax - ymin), dim=1)

def prepare_for_coco_detection(predictions):
    """
    å°†é¢„æµ‹ç»“æœè½¬æ¢ä¸ºCOCOæ ‡å‡†æ ¼å¼

    Args:
        predictions: Dict[int, Dict[str, Tensor]]
            {image_id: {'boxes': Tensor[N,4], 'scores': Tensor[N], 'labels': Tensor[N]}}

    Returns:
        List[Dict] COCOæ ¼å¼çš„æ£€æµ‹ç»“æœ
    """
    coco_results = []
    for image_id, prediction in predictions.items():
        if len(prediction["boxes"]) == 0:
            continue

        boxes = convert_to_xywh(prediction["boxes"]).tolist()
        scores = prediction["scores"].tolist()
        labels = prediction["labels"].tolist()

        coco_results.extend([
            {
                "image_id": image_id,
                "category_id": labels[k],
                "bbox": box,
                "score": scores[k],
            }
            for k, box in enumerate(boxes)
        ])

    return coco_results

def coco_extended_metrics(coco_eval, coco=None):
    """
    è®¡ç®—æ‰©å±•çš„COCOæŒ‡æ ‡

    (å®Œæ•´ä»£ç è§ç¬¬5.2èŠ‚)
    """
    # ... (å¤åˆ¶RF-DETRçš„å®ç°)
    pass
```

### 7.3 é€‚é…ç°æœ‰BaseOnnxè¾“å‡º

ä½ çš„é¡¹ç›®ä¸­`BaseOnnx`çš„è¾“å‡ºéœ€è¦è½¬æ¢ä¸ºCOCOæ ¼å¼ï¼š

```python
# infer_onnx/base_onnx.py æˆ– tools/eval.py

def convert_baseonnx_output_to_coco_format(model_output, image_id, orig_size):
    """
    å°†BaseOnnxçš„è¾“å‡ºè½¬æ¢ä¸ºCOCOè¯„ä¼°æ ¼å¼

    Args:
        model_output: BaseOnnxçš„è¾“å‡º (æ¯ä¸ªå­ç±»æ ¼å¼å¯èƒ½ä¸åŒ)
        image_id: å›¾åƒID (int)
        orig_size: åŸå›¾å°ºå¯¸ (height, width)

    Returns:
        Dict åŒ…å« {'boxes': Tensor[N,4], 'scores': Tensor[N], 'labels': Tensor[N]}
    """
    # ç¤ºä¾‹: å‡è®¾BaseOnnxè¾“å‡ºæ˜¯ä¸€ä¸ªå­—å…¸
    # model_output = {
    #     'boxes': np.ndarray[N, 4],  (å¯èƒ½æ˜¯xyxyæˆ–xywhæ ¼å¼)
    #     'scores': np.ndarray[N],
    #     'labels': np.ndarray[N]
    # }

    boxes = torch.from_numpy(model_output['boxes'])
    scores = torch.from_numpy(model_output['scores'])
    labels = torch.from_numpy(model_output['labels'])

    # ç¡®ä¿boxesæ˜¯xyxyæ ¼å¼çš„ç»å¯¹åæ ‡
    # (å¦‚æœæ˜¯å½’ä¸€åŒ–åæ ‡ï¼Œéœ€è¦ç¼©æ”¾åˆ°ç»å¯¹åæ ‡)
    if boxes.max() <= 1.0:
        h, w = orig_size
        scale = torch.tensor([w, h, w, h])
        boxes = boxes * scale

    # å¦‚æœboxesæ˜¯xywhæ ¼å¼ï¼Œè½¬æ¢ä¸ºxyxy
    # boxes = xywh_to_xyxy(boxes)

    return {
        'boxes': boxes,
        'scores': scores,
        'labels': labels
    }

# ä½¿ç”¨ç¤ºä¾‹
for images, targets in dataloader:
    outputs = detector(images)  # BaseOnnx.__call__()

    results = {
        target['image_id'].item(): convert_baseonnx_output_to_coco_format(
            outputs[i],
            target['image_id'].item(),
            target['orig_size']
        )
        for i, target in enumerate(targets)
    }

    coco_evaluator.update(results)
```

### 7.4 å‘½ä»¤è¡Œå·¥å…·ç¤ºä¾‹

```python
# tools/eval.py

import argparse
from pathlib import Path
from infer_onnx import create_detector
from utils.coco_metrics import CocoEvaluator, coco_extended_metrics
from pycocotools.coco import COCO

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--model-path', type=str, required=True)
    parser.add_argument('--model-type', type=str, choices=['yolo', 'rtdetr', 'rfdetr'])
    parser.add_argument('--dataset-path', type=str, required=True)
    parser.add_argument('--output-dir', type=str, default='./runs/eval')
    parser.add_argument('--conf-threshold', type=float, default=0.25)
    args = parser.parse_args()

    # åŠ è½½æ¨¡å‹
    detector = create_detector(
        model_path=args.model_path,
        model_type=args.model_type,
        conf_threshold=args.conf_threshold
    )

    # åŠ è½½COCOæ•°æ®é›†
    coco_gt = COCO(f"{args.dataset_path}/annotations/instances_val2017.json")
    dataloader = create_dataloader(args.dataset_path)

    # è¯„ä¼°
    coco_evaluator = CocoEvaluator(coco_gt, iou_types=['bbox'])

    for images, targets in dataloader:
        outputs = detector(images)
        results = convert_outputs_to_coco_format(outputs, targets)
        coco_evaluator.update(results)

    # ç´¯ç§¯å¹¶æ±‡æ€»
    coco_evaluator.accumulate()
    coco_evaluator.summarize()

    # è®¡ç®—æ‰©å±•æŒ‡æ ‡
    metrics = coco_extended_metrics(coco_evaluator.coco_eval['bbox'], coco=coco_gt)

    # ä¿å­˜ç»“æœ
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    with open(output_dir / "metrics.json", "w") as f:
        json.dump(metrics, f, indent=4)

    print(f"\nè¯„ä¼°å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: {output_dir / 'metrics.json'}")
    print(f"mAP@50:95 = {metrics['map@50:95']:.3f}")
    print(f"Precision = {metrics['precision']:.3f}")
    print(f"Recall    = {metrics['recall']:.3f}")
    print(f"F1        = {metrics['f1']:.3f}")

if __name__ == '__main__':
    main()
```

---

## 8. æ€»ç»“ä¸å»ºè®®

### 8.1 æ ¸å¿ƒå‘ç°

1. **æ•°æ®æµè½¬å…³é”®è·¯å¾„**:
   ```
   æ¨¡å‹è¾“å‡º (ç›¸å¯¹åæ ‡, cxcywh)
     â†’ PostProcess (ç»å¯¹åæ ‡, xyxy)
     â†’ å­—å…¸æ ¼å¼ {image_id: prediction}
     â†’ COCO JSONæ ¼å¼ (xywh)
     â†’ pycocotoolsè¯„ä¼°
   ```

2. **è¾¹æ¡†æ ¼å¼å¤šæ¬¡è½¬æ¢**:
   - æ¨¡å‹è¾“å‡º: `[cx, cy, w, h]` (0-1å½’ä¸€åŒ–)
   - PostProcess: `[x1, y1, x2, y2]` (ç»å¯¹åƒç´ )
   - COCO API: `[x, y, w, h]` (ç»å¯¹åƒç´ )

3. **RF-DETRçš„ä¼˜åŠ¿**:
   - æä¾›å®Œæ•´çš„`CocoEvaluator`å°è£…
   - æ”¯æŒåˆ†å¸ƒå¼è®­ç»ƒè¯„ä¼°
   - é¢å¤–æä¾›Precision/Recall/F1æŒ‡æ ‡
   - æŒ‰ç±»åˆ«ç»†åˆ†æ€§èƒ½åˆ†æ

### 8.2 é›†æˆå»ºè®®ä¼˜å…ˆçº§

**é«˜ä¼˜å…ˆçº§** (ç«‹å³å¯ç”¨):
1. âœ… å¤åˆ¶`convert_to_xywh`å‡½æ•°åˆ°é¡¹ç›®ä¸­
2. âœ… å¤åˆ¶`prepare_for_coco_detection`å‡½æ•°
3. âœ… ç¼–å†™é€‚é…å™¨å‡½æ•°å°†BaseOnnxè¾“å‡ºè½¬æ¢ä¸ºCOCOæ ¼å¼

**ä¸­ä¼˜å…ˆçº§** (å¢å¼ºåŠŸèƒ½):
4. âš¡ é›†æˆ`CocoEvaluator`ç±»å®Œæ•´å®ç°
5. âš¡ æ·»åŠ `coco_extended_metrics`å‡½æ•°
6. âš¡ åˆ›å»ºå‘½ä»¤è¡Œè¯„ä¼°å·¥å…·

**ä½ä¼˜å…ˆçº§** (å¯é€‰ä¼˜åŒ–):
7. ğŸ”§ æ”¯æŒåˆ†å¸ƒå¼è¯„ä¼°
8. ğŸ”§ æ·»åŠ åˆ†å‰²ä»»åŠ¡è¯„ä¼° (å¦‚æœéœ€è¦)
9. ğŸ”§ å¯è§†åŒ–æ¯ä¸ªç±»åˆ«çš„æ€§èƒ½

### 8.3 å…³é”®æ³¨æ„äº‹é¡¹

1. **åæ ‡ç³»ç»Ÿä¸€è‡´æ€§**:
   - ç¡®ä¿ä½ çš„æ¨¡å‹è¾“å‡ºä½¿ç”¨çš„åæ ‡ç³»ä¸PostProcesså‡è®¾ä¸€è‡´
   - æ£€æŸ¥æ˜¯å¦éœ€è¦å½’ä¸€åŒ–æˆ–åå½’ä¸€åŒ–

2. **ç±»åˆ«IDæ˜ å°„**:
   - COCOç±»åˆ«IDæ˜¯1-based (1=person, 2=bicycle, ...)
   - æ¨¡å‹è¾“å‡ºå¯èƒ½æ˜¯0-basedï¼Œéœ€è¦æ³¨æ„è½¬æ¢

3. **ç½®ä¿¡åº¦é˜ˆå€¼**:
   - PostProcessé»˜è®¤é€‰æ‹©Top-100é¢„æµ‹
   - å¯ä»¥æ ¹æ®å®é™…éœ€æ±‚è°ƒæ•´`num_select`å‚æ•°

4. **æ€§èƒ½è€ƒè™‘**:
   - COCOè¯„ä¼°åœ¨å¤§è§„æ¨¡æ•°æ®é›†ä¸Šå¯èƒ½è¾ƒæ…¢
   - è€ƒè™‘ä½¿ç”¨å¤šè¿›ç¨‹åŠ é€Ÿ (RF-DETRå·²æ”¯æŒ)

### 8.4 ä¸‹ä¸€æ­¥è¡ŒåŠ¨

1. **ç«‹å³è¡ŒåŠ¨**:
   - å¤åˆ¶å…³é”®å‡½æ•°åˆ° `utils/coco_metrics.py`
   - ç¼–å†™é€‚é…å™¨è¿æ¥BaseOnnxå’ŒCocoEvaluator
   - è¿è¡Œå°è§„æ¨¡æµ‹è¯•éªŒè¯æµç¨‹æ­£ç¡®

2. **çŸ­æœŸè®¡åˆ’**:
   - é›†æˆå®Œæ•´çš„CocoEvaluatorç±»
   - åœ¨ `tools/eval.py` æ·»åŠ COCOè¯„ä¼°é€‰é¡¹
   - éªŒè¯è¯„ä¼°ç»“æœä¸Ultralyticsä¸€è‡´æ€§

3. **é•¿æœŸä¼˜åŒ–**:
   - æ·»åŠ å¯è§†åŒ–å·¥å…·å±•ç¤ºæ¯ä¸ªç±»åˆ«çš„æ€§èƒ½
   - æ”¯æŒè‡ªå®šä¹‰æ•°æ®é›†è¯„ä¼°
   - é›†æˆåˆ°CI/CDæµç¨‹ä¸­

---

## 9. é™„å½•

### 9.1 ç›¸å…³æ–‡ä»¶è·¯å¾„æ¸…å•

```
third_party/rfdetr/
â”œâ”€â”€ datasets/
â”‚   â”œâ”€â”€ coco_eval.py              # CocoEvaluatoræ ¸å¿ƒç±» (272è¡Œ)
â”‚   â””â”€â”€ coco.py                   # COCOæ•°æ®é›†åŠ è½½ (330è¡Œ)
â”œâ”€â”€ models/
â”‚   â””â”€â”€ lwdetr.py                 # PostProcessç±» (1000+è¡Œ)
â”œâ”€â”€ engine.py                     # è¯„ä¼°ä¸»å¾ªç¯å’Œæ‰©å±•æŒ‡æ ‡ (341è¡Œ)
â”œâ”€â”€ util/
â”‚   â””â”€â”€ metrics.py                # æŒ‡æ ‡å¯è§†åŒ– (243è¡Œ)
â””â”€â”€ detr.py                       # é«˜çº§API (477è¡Œ)
```

### 9.2 pycocotools APIå‚è€ƒ

```python
# COCOç±» (Ground Truthç®¡ç†)
coco = COCO(annotation_file)
coco.getAnnIds(imgIds, catIds, iscrowd)
coco.loadAnns(ids)
coco.loadCats(ids)
coco.loadImgs(ids)
coco.loadRes(results)  # åŠ è½½é¢„æµ‹ç»“æœ

# COCOevalç±» (è¯„ä¼°å¼•æ“)
coco_eval = COCOeval(cocoGt, cocoDt, iouType)
coco_eval.params.imgIds = [...]
coco_eval.params.catIds = [...]
coco_eval.evaluate()
coco_eval.accumulate()
coco_eval.summarize()

# å…³é”®å‚æ•°
coco_eval.params.iouThrs = [0.50:0.05:0.95]  # IoUé˜ˆå€¼
coco_eval.params.recThrs = [0.00:0.01:1.00]  # å¬å›é˜ˆå€¼
coco_eval.params.maxDets = [1, 10, 100]      # æœ€å¤§æ£€æµ‹æ•°
coco_eval.params.areaRng = [[0**2, 1e5**2], [0**2, 32**2], [32**2, 96**2], [96**2, 1e5**2]]
```

### 9.3 å¸¸è§é—®é¢˜æ’æŸ¥

**Q1: è¯„ä¼°ç»“æœä¸º0æˆ–NaN**
- æ£€æŸ¥è¾¹æ¡†æ ¼å¼æ˜¯å¦æ­£ç¡® (xyxy vs xywh)
- éªŒè¯ç±»åˆ«IDæ˜¯å¦åŒ¹é… (0-based vs 1-based)
- ç¡®è®¤åæ ‡æ˜¯å¦åœ¨æœ‰æ•ˆèŒƒå›´å†…

**Q2: COCO.loadResæŠ¥é”™**
- æ£€æŸ¥resultsæ ¼å¼æ˜¯å¦ç¬¦åˆCOCOæ ‡å‡†
- ç¡®ä¿image_idåœ¨ground truthä¸­å­˜åœ¨
- éªŒè¯bboxä¸ä¸ºç©ºæˆ–æ— æ•ˆå€¼

**Q3: è¯„ä¼°é€Ÿåº¦æ…¢**
- å‡å°‘num_selectå‚æ•° (é»˜è®¤100)
- ä½¿ç”¨å¤šè¿›ç¨‹åŠ è½½æ•°æ®
- è€ƒè™‘ä½¿ç”¨RF-DETRçš„åˆ†å¸ƒå¼è¯„ä¼°

---

**æŠ¥å‘Šå®Œæˆæ—¶é—´**: 2025-10-11
**åˆ†æä»£ç è¡Œæ•°**: ~2500è¡Œ
**æ¶‰åŠæ–‡ä»¶æ•°é‡**: 22ä¸ªPythonæ–‡ä»¶
**å»ºè®®é›†æˆæ—¶é—´**: 1-2ä¸ªå·¥ä½œæ—¥
**é¢„æœŸå·¥ä½œé‡**: ä¸­ç­‰å¤æ‚åº¦ (éœ€è¦ç†è§£åæ ‡ç³»ç»Ÿå’Œæ ¼å¼è½¬æ¢)
