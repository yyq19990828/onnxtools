[project]
name = "onnxtools"
version = "0.1.0"
description = "ONNX model inference toolkit with multi-architecture support"
readme = "README.md"
requires-python = ">=3.10"
dependencies = [
    "colored>=2.3.1",
    "colorlog>=6.9.0",
    "matplotlib>=3.10.6",
    "numpy>=2.2.6",
    "onnx==1.17.0",
    "onnx-graphsurgeon",
    # 不使用 [cuda,cudnn] extras，避免自动安装 nvidia-* python包
    # 系统应该已经安装了 CUDA 和 cuDNN 库（通过 apt/yum 或手动安装）
    "onnxruntime-gpu==1.22.0",
    "onnxslim>=0.1.65",
    "opencv-contrib-python>=4.12.0.88",
    "pillow>=11.3.0",
    "polygraphy>=0.49.26",
    "pytest-cov>=7.0.0",
    "python-levenshtein>=0.25.0",
    "pyyaml>=6.0.2",
    "supervision==0.26.1",
]

[project.optional-dependencies]
# TensorRT加速支持 (可选)
# 安装方式: uv pip install -e ".[trt]"
# 注意: 需要先安装 pip setuptools wheel: uv pip install pip setuptools wheel
trt = [
    "tensorrt==8.6.1.post1",
    "tensorrt-bindings==8.6.1",
    "tensorrt-libs==8.6.1",
]

# MCP (Model Context Protocol) 支持 (可选)
# 安装方式: uv pip install -e ".[mcp]"
# 用于将 onnxtools 功能暴露给 LLM (如 Claude)
mcp = [
    "mcp>=1.0.0",
    "httpx>=0.27.0",
]

[project.scripts]
# MCP 服务器入口点
onnxtools-mcp = "mcp_tools.server:main"

[build-system]
requires = ["setuptools>=61.0", "wheel"]
build-backend = "setuptools.build_meta"

[tool.setuptools.packages.find]
include = ["onnxtools*", "mcp_tools*"]
exclude = ["tests*", "docs*", "data*", "specs*", "models*", "configs*", "tools*"]

[tool.uv]
# 配置包索引源
index-url = "https://pypi.org/simple/"
extra-index-url = [
    # NVIDIA NGC PyPI 镜像，用于TensorRT相关包
    "https://pypi.nvidia.com",
]

# TensorRT包需要禁用构建隔离，因为它的安装脚本依赖pip模块
no-build-isolation-package = [
    "tensorrt",
    "tensorrt-bindings",
    "tensorrt-libs"
]

[tool.uv.workspace]
members = ["mcp_vehicle_detection"]
